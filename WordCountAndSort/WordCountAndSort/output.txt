Deep
learning
From
Wikipedia
the
free
encyclopedia
For
deep
versus
shallow
learning
in
educational
psychology
see
Student
approaches
to
learning
For
more
information
see
Artificial
neural
network
Machine
learning
and
data
mining
Kernel
Machine
svg
Problems
show
Supervised
learning
classification
regression
show
Clustering
show
Dimensionality
reduction
show
Structured
prediction
show
Anomaly
detection
show
Neural
nets
show
Reinforcement
learning
show
Theory
show
Machine
learning
venues
show
Related
articles
show
Portal
puzzle
svg
Machine
learning
portal
v
t
e
Deep
learning
also
known
as
deep
structured
learning
or
hierarchical
learning
is
part
of
a
broader
family
of
machine
learning
methods
based
on
learning
data
representations
as
opposed
to
task
specific
algorithms
Learning
can
be
supervised
semi
supervised
or
unsupervised
Deep
learning
architectures
such
as
deep
neural
networks
deep
belief
networks
and
recurrent
neural
networks
have
been
applied
to
fields
including
computer
vision
speech
recognition
natural
language
processing
audio
recognition
social
network
filtering
machine
translation
bioinformatics
and
drug
design
where
they
have
produced
results
comparable
to
and
in
some
cases
superior
to
human
experts
Deep
learning
models
are
vaguely
inspired
by
information
processing
and
communication
patterns
in
biological
nervous
systems
yet
have
various
differences
from
the
structural
and
functional
properties
of
biological
brains
which
make
them
incompatible
with
neuroscience
evidences
Contents
Definition
Overview
Interpretations
History
Deep
learning
revolution
Artificial
neural
networks
Deep
neural
networks
Challenges
Applications
Automatic
speech
recognition
Image
recognition
Visual
art
processing
Natural
language
processing
Reliability
of
infrastructure
systems
Drug
discovery
and
toxicology
Customer
relationship
management
Recommendation
systems
Bioinformatics
Mobile
advertising
Image
restoration
Relation
to
human
cognitive
and
brain
development
Commercial
activity
Criticism
and
comment
Theory
Errors
Cyberthreat
See
also
References
External
links
Definition
Deep
learning
is
a
class
of
machine
learning
algorithms
that
pp
C
use
a
cascade
of
multiple
layers
of
nonlinear
processing
units
for
feature
extraction
and
transformation
Each
successive
layer
uses
the
output
from
the
previous
layer
as
input
learn
in
supervised
e
g
classification
and
or
unsupervised
e
g
pattern
analysis
manners
learn
multiple
levels
of
representations
that
correspond
to
different
levels
of
abstraction
the
levels
form
a
hierarchy
of
concepts
Overview
Most
modern
deep
learning
models
are
based
on
an
artificial
neural
network
although
they
can
also
include
propositional
formulas
or
latent
variables
organized
layer
wise
in
deep
generative
models
such
as
the
nodes
in
Deep
Belief
Networks
and
Deep
Boltzmann
Machines
In
deep
learning
each
level
learns
to
transform
its
input
data
into
a
slightly
more
abstract
and
composite
representation
In
an
image
recognition
application
the
raw
input
may
be
a
matrix
of
pixels
the
first
representational
layer
may
abstract
the
pixels
and
encode
edges
the
second
layer
may
compose
and
encode
arrangements
of
edges
the
third
layer
may
encode
a
nose
and
eyes
and
the
fourth
layer
may
recognize
that
the
image
contains
a
face
Importantly
a
deep
learning
process
can
learn
which
features
to
optimally
place
in
which
level
on
its
own
Of
course
this
does
not
completely
obviate
the
need
for
hand
tuning
for
example
varying
numbers
of
layers
and
layer
sizes
can
provide
different
degrees
of
abstraction
The
deep
in
deep
learning
refers
to
the
number
of
layers
through
which
the
data
is
transformed
More
precisely
deep
learning
systems
have
a
substantial
credit
assignment
path
CAP
depth
The
CAP
is
the
chain
of
transformations
from
input
to
output
CAPs
describe
potentially
causal
connections
between
input
and
output
For
a
feedforward
neural
network
the
depth
of
the
CAPs
is
that
of
the
network
and
is
the
number
of
hidden
layers
plus
one
as
the
output
layer
is
also
parameterized
For
recurrent
neural
networks
in
which
a
signal
may
propagate
through
a
layer
more
than
once
the
CAP
depth
is
potentially
unlimited
No
universally
agreed
upon
threshold
of
depth
divides
shallow
learning
from
deep
learning
but
most
researchers
agree
that
deep
learning
involves
CAP
depth
CAP
of
depth
has
been
shown
to
be
a
universal
approximator
in
the
sense
that
it
can
emulate
any
function
citation
needed
Beyond
that
more
layers
do
not
add
to
the
function
approximator
ability
of
the
network
The
extra
layers
help
in
learning
features
Deep
learning
architectures
are
often
constructed
with
a
greedy
layer
by
layer
method
clarification
needed
further
explanation
needed
citation
needed
Deep
learning
helps
to
disentangle
these
abstractions
and
pick
out
which
features
improve
performance
For
supervised
learning
tasks
deep
learning
methods
obviate
feature
engineering
by
translating
the
data
into
compact
intermediate
representations
akin
to
principal
components
and
derive
layered
structures
that
remove
redundancy
in
representation
Deep
learning
algorithms
can
be
applied
to
unsupervised
learning
tasks
This
is
an
important
benefit
because
unlabeled
data
are
more
abundant
than
labeled
data
Examples
of
deep
structures
that
can
be
trained
in
an
unsupervised
manner
are
neural
history
compressors
and
deep
belief
networks
Interpretations
Deep
neural
networks
are
generally
interpreted
in
terms
of
the
universal
approximation
theorem
or
probabilistic
inference
The
universal
approximation
theorem
concerns
the
capacity
of
feedforward
neural
networks
with
a
single
hidden
layer
of
finite
size
to
approximate
continuous
functions
In
the
first
proof
was
published
by
George
Cybenko
for
sigmoid
activation
functions
and
was
generalised
to
feed
forward
multi
layer
architectures
in
by
Kurt
Hornik
The
probabilistic
interpretation
derives
from
the
field
of
machine
learning
It
features
inference
as
well
as
the
optimization
concepts
of
training
and
testing
related
to
fitting
and
generalization
respectively
More
specifically
the
probabilistic
interpretation
considers
the
activation
nonlinearity
as
a
cumulative
distribution
function
The
probabilistic
interpretation
led
to
the
introduction
of
dropout
as
regularizer
in
neural
networks
The
probabilistic
interpretation
was
introduced
by
researchers
including
Hopfield
Widrow
and
Narendra
and
popularized
in
surveys
such
as
the
one
by
Bishop
History
The
term
Deep
Learning
was
introduced
to
the
machine
learning
community
by
Rina
Dechter
in
and
to
Artificial
Neural
Networks
by
Igor
Aizenberg
and
colleagues
in
in
the
context
of
Boolean
threshold
neurons
The
first
general
working
learning
algorithm
for
supervised
deep
feedforward
multilayer
perceptrons
was
published
by
Alexey
Ivakhnenko
and
Lapa
in
A
paper
described
a
deep
network
with
layers
trained
by
the
group
method
of
data
handling
algorithm
Other
deep
learning
working
architectures
specifically
those
built
for
computer
vision
began
with
the
Neocognitron
introduced
by
Kunihiko
Fukushima
in
In
Yann
LeCun
et
al
applied
the
standard
backpropagation
algorithm
which
had
been
around
as
the
reverse
mode
of
automatic
differentiation
since
to
a
deep
neural
network
with
the
purpose
of
recognizing
handwritten
ZIP
codes
on
mail
While
the
algorithm
worked
training
required
days
By
such
systems
were
used
for
recognizing
isolated
D
hand
written
digits
while
recognizing
D
objects
was
done
by
matching
D
images
with
a
handcrafted
D
object
model
Weng
et
al
suggested
that
a
human
brain
does
not
use
a
monolithic
D
object
model
and
in
they
published
Cresceptron
a
method
for
performing
D
object
recognition
in
cluttered
scenes
Cresceptron
is
a
cascade
of
layers
similar
to
Neocognitron
But
while
Neocognitron
required
a
human
programmer
to
hand
merge
features
Cresceptron
learned
an
open
number
of
features
in
each
layer
without
supervision
where
each
feature
is
represented
by
a
convolution
kernel
Cresceptron
segmented
each
learned
object
from
a
cluttered
scene
through
back
analysis
through
the
network
Max
pooling
now
often
adopted
by
deep
neural
networks
e
g
ImageNet
tests
was
first
used
in
Cresceptron
to
reduce
the
position
resolution
by
a
factor
of
x
to
through
the
cascade
for
better
generalization
In
Andr
de
Carvalho
together
with
Mike
Fairhurst
and
David
Bisset
published
experimental
results
of
a
multi
layer
boolean
neural
network
also
known
as
a
weightless
neural
network
composed
of
a
layers
self
organising
feature
extraction
neural
network
module
SOFT
followed
by
a
multi
layer
classification
neural
network
module
GSN
which
were
independently
trained
Each
layer
in
the
feature
extraction
module
extracted
features
with
growing
complexity
regarding
the
previous
layer
In
Brendan
Frey
demonstrated
that
it
was
possible
to
train
over
two
days
a
network
containing
six
fully
connected
layers
and
several
hundred
hidden
units
using
the
wake
sleep
algorithm
co
developed
with
Peter
Dayan
and
Hinton
Many
factors
contribute
to
the
slow
speed
including
the
vanishing
gradient
problem
analyzed
in
by
Sepp
Hochreiter
Simpler
models
that
use
task
specific
handcrafted
features
such
as
Gabor
filters
and
support
vector
machines
SVMs
were
a
popular
choice
in
the
s
and
s
because
of
ANNs
computational
cost
and
a
lack
of
understanding
of
how
the
brain
wires
its
biological
networks
Both
shallow
and
deep
learning
e
g
recurrent
nets
of
ANNs
have
been
explored
for
many
years
These
methods
never
outperformed
non
uniform
internal
handcrafting
Gaussian
mixture
model
Hidden
Markov
model
GMM
HMM
technology
based
on
generative
models
of
speech
trained
discriminatively
Key
difficulties
have
been
analyzed
including
gradient
diminishing
and
weak
temporal
correlation
structure
in
neural
predictive
models
Additional
difficulties
were
the
lack
of
training
data
and
limited
computing
power
Most
speech
recognition
researchers
moved
away
from
neural
nets
to
pursue
generative
modeling
An
exception
was
at
SRI
International
in
the
late
s
Funded
by
the
US
government
s
NSA
and
DARPA
SRI
studied
deep
neural
networks
in
speech
and
speaker
recognition
Heck
s
speaker
recognition
team
achieved
the
first
significant
success
with
deep
neural
networks
in
speech
processing
in
the
National
Institute
of
Standards
and
Technology
Speaker
Recognition
evaluation
While
SRI
experienced
success
with
deep
neural
networks
in
speaker
recognition
they
were
unsuccessful
in
demonstrating
similar
success
in
speech
recognition
The
principle
of
elevating
raw
features
over
hand
crafted
optimization
was
first
explored
successfully
in
the
architecture
of
deep
autoencoder
on
the
raw
spectrogram
or
linear
filter
bank
features
in
the
late
s
showing
its
superiority
over
the
Mel
Cepstral
features
that
contain
stages
of
fixed
transformation
from
spectrograms
The
raw
features
of
speech
waveforms
later
produced
excellent
larger
scale
results
Many
aspects
of
speech
recognition
were
taken
over
by
a
deep
learning
method
called
Long
short
term
memory
LSTM
a
recurrent
neural
network
published
by
Hochreiter
and
Schmidhuber
in
LSTM
RNNs
avoid
the
vanishing
gradient
problem
and
can
learn
Very
Deep
Learning
tasks
that
require
memories
of
events
that
happened
thousands
of
discrete
time
steps
before
which
is
important
for
speech
In
LSTM
started
to
become
competitive
with
traditional
speech
recognizers
on
certain
tasks
Later
it
was
combined
with
connectionist
temporal
classification
CTC
in
stacks
of
LSTM
RNNs
In
Google
s
speech
recognition
reportedly
experienced
a
dramatic
performance
jump
of
through
CTC
trained
LSTM
which
they
made
available
through
Google
Voice
Search
In
publications
by
Geoff
Hinton
Ruslan
Salakhutdinov
Osindero
and
Teh
showed
how
a
many
layered
feedforward
neural
network
could
be
effectively
pre
trained
one
layer
at
a
time
treating
each
layer
in
turn
as
an
unsupervised
restricted
Boltzmann
machine
then
fine
tuning
it
using
supervised
backpropagation
The
papers
referred
to
learning
for
deep
belief
nets
Deep
learning
is
part
of
state
of
the
art
systems
in
various
disciplines
particularly
computer
vision
and
automatic
speech
recognition
ASR
Results
on
commonly
used
evaluation
sets
such
as
TIMIT
ASR
and
MNIST
image
classification
as
well
as
a
range
of
large
vocabulary
speech
recognition
tasks
have
steadily
improved
Convolutional
neural
networks
CNNs
were
superseded
for
ASR
by
CTC
for
LSTM
but
are
more
successful
in
computer
vision
The
impact
of
deep
learning
in
industry
began
in
the
early
s
when
CNNs
already
processed
an
estimated
to
of
all
the
checks
written
in
the
US
according
to
Yann
LeCun
Industrial
applications
of
deep
learning
to
large
scale
speech
recognition
started
around
The
NIPS
Workshop
on
Deep
Learning
for
Speech
Recognition
was
motivated
by
the
limitations
of
deep
generative
models
of
speech
and
the
possibility
that
given
more
capable
hardware
and
large
scale
data
sets
that
deep
neural
nets
DNN
might
become
practical
It
was
believed
that
pre
training
DNNs
using
generative
models
of
deep
belief
nets
DBN
would
overcome
the
main
difficulties
of
neural
nets
However
it
was
discovered
that
replacing
pre
training
with
large
amounts
of
training
data
for
straightforward
backpropagation
when
using
DNNs
with
large
context
dependent
output
layers
produced
error
rates
dramatically
lower
than
then
state
of
the
art
Gaussian
mixture
model
GMM
Hidden
Markov
Model
HMM
and
also
than
more
advanced
generative
model
based
systems
The
nature
of
the
recognition
errors
produced
by
the
two
types
of
systems
was
characteristically
different
offering
technical
insights
into
how
to
integrate
deep
learning
into
the
existing
highly
efficient
run
time
speech
decoding
system
deployed
by
all
major
speech
recognition
systems
Analysis
around
contrasted
the
GMM
and
other
generative
speech
models
vs
DNN
models
stimulated
early
industrial
investment
in
deep
learning
for
speech
recognition
eventually
leading
to
pervasive
and
dominant
use
in
that
industry
That
analysis
was
done
with
comparable
performance
less
than
in
error
rate
between
discriminative
DNNs
and
generative
models
In
researchers
extended
deep
learning
from
TIMIT
to
large
vocabulary
speech
recognition
by
adopting
large
output
layers
of
the
DNN
based
on
context
dependent
HMM
states
constructed
by
decision
trees
Advances
in
hardware
enabled
the
renewed
interest
In
Nvidia
was
involved
in
what
was
called
the
big
bang
of
deep
learning
as
deep
learning
neural
networks
were
trained
with
Nvidia
graphics
processing
units
GPUs
That
year
Google
Brain
used
Nvidia
GPUs
to
create
capable
DNNs
While
there
Ng
determined
that
GPUs
could
increase
the
speed
of
deep
learning
systems
by
about
times
In
particular
GPUs
are
well
suited
for
the
matrix
vector
math
involved
in
machine
learning
GPUs
speed
up
training
algorithms
by
orders
of
magnitude
reducing
running
times
from
weeks
to
days
Specialized
hardware
and
algorithm
optimizations
can
be
used
for
efficient
processing
Deep
learning
revolution
In
a
team
led
by
Dahl
won
the
Merck
Molecular
Activity
Challenge
using
multi
task
deep
neural
networks
to
predict
the
biomolecular
target
of
one
drug
In
Hochreiter
s
group
used
deep
learning
to
detect
off
target
and
toxic
effects
of
environmental
chemicals
in
nutrients
household
products
and
drugs
and
won
the
Tox
Data
Challenge
of
NIH
FDA
and
NCATS
Significant
additional
impacts
in
image
or
object
recognition
were
felt
from
to
Although
CNNs
trained
by
backpropagation
had
been
around
for
decades
and
GPU
implementations
of
NNs
for
years
including
CNNs
fast
implementations
of
CNNs
with
max
pooling
on
GPUs
in
the
style
of
Ciresan
and
colleagues
were
needed
to
progress
on
computer
vision
In
this
approach
achieved
for
the
first
time
superhuman
performance
in
a
visual
pattern
recognition
contest
Also
in
it
won
the
ICDAR
Chinese
handwriting
contest
and
in
May
it
won
the
ISBI
image
segmentation
contest
Until
CNNs
did
not
play
a
major
role
at
computer
vision
conferences
but
in
June
a
paper
by
Ciresan
et
al
at
the
leading
conference
CVPR
showed
how
max
pooling
CNNs
on
GPU
can
dramatically
improve
many
vision
benchmark
records
In
October
a
similar
system
by
Krizhevsky
et
al
won
the
large
scale
ImageNet
competition
by
a
significant
margin
over
shallow
machine
learning
methods
In
November
Ciresan
et
al
s
system
also
won
the
ICPR
contest
on
analysis
of
large
medical
images
for
cancer
detection
and
in
the
following
year
also
the
MICCAI
Grand
Challenge
on
the
same
topic
In
and
the
error
rate
on
the
ImageNet
task
using
deep
learning
was
further
reduced
following
a
similar
trend
in
large
scale
speech
recognition
The
Wolfram
Image
Identification
project
publicized
these
improvements
Image
classification
was
then
extended
to
the
more
challenging
task
of
generating
descriptions
captions
for
images
often
as
a
combination
of
CNNs
and
LSTMs
Some
researchers
assess
that
the
October
ImageNet
victory
anchored
the
start
of
a
deep
learning
revolution
that
has
transformed
the
AI
industry
Artificial
neural
networks
Main
article
Artificial
neural
network
Artificial
neural
networks
ANNs
or
connectionist
systems
are
computing
systems
inspired
by
the
biological
neural
networks
that
constitute
animal
brains
Such
systems
learn
progressively
improve
their
ability
to
do
tasks
by
considering
examples
generally
without
task
specific
programming
For
example
in
image
recognition
they
might
learn
to
identify
images
that
contain
cats
by
analyzing
example
images
that
have
been
manually
labeled
as
cat
or
no
cat
and
using
the
analytic
results
to
identify
cats
in
other
images
They
have
found
most
use
in
applications
difficult
to
express
with
a
traditional
computer
algorithm
using
rule
based
programming
An
ANN
is
based
on
a
collection
of
connected
units
called
artificial
neurons
analogous
to
axons
in
a
biological
brain
Each
connection
synapse
between
neurons
can
transmit
a
signal
to
another
neuron
The
receiving
postsynaptic
neuron
can
process
the
signal
s
and
then
signal
downstream
neurons
connected
to
it
Neurons
may
have
state
generally
represented
by
real
numbers
typically
between
and
Neurons
and
synapses
may
also
have
a
weight
that
varies
as
learning
proceeds
which
can
increase
or
decrease
the
strength
of
the
signal
that
it
sends
downstream
Typically
neurons
are
organized
in
layers
Different
layers
may
perform
different
kinds
of
transformations
on
their
inputs
Signals
travel
from
the
first
input
to
the
last
output
layer
possibly
after
traversing
the
layers
multiple
times
The
original
goal
of
the
neural
network
approach
was
to
solve
problems
in
the
same
way
that
a
human
brain
would
Over
time
attention
focused
on
matching
specific
mental
abilities
leading
to
deviations
from
biology
such
as
backpropagation
or
passing
information
in
the
reverse
direction
and
adjusting
the
network
to
reflect
that
information
Neural
networks
have
been
used
on
a
variety
of
tasks
including
computer
vision
speech
recognition
machine
translation
social
network
filtering
playing
board
and
video
games
and
medical
diagnosis
As
of
neural
networks
typically
have
a
few
thousand
to
a
few
million
units
and
millions
of
connections
Despite
this
number
being
several
order
of
magnitude
less
than
the
number
of
neurons
on
a
human
brain
these
networks
can
perform
many
tasks
at
a
level
beyond
that
of
humans
e
g
recognizing
faces
playing
Go
Deep
neural
networks
This
section
may
be
too
technical
for
most
readers
to
understand
Please
help
improve
it
to
make
it
understandable
to
non
experts
without
removing
the
technical
details
July
Learn
how
and
when
to
remove
this
template
message
A
deep
neural
network
DNN
is
an
artificial
neural
network
ANN
with
multiple
hidden
layers
between
the
input
and
output
layers
DNNs
can
model
complex
non
linear
relationships
DNN
architectures
generate
compositional
models
where
the
object
is
expressed
as
a
layered
composition
of
primitives
The
extra
layers
enable
composition
of
features
from
lower
layers
potentially
modeling
complex
data
with
fewer
units
than
a
similarly
performing
shallow
network
Deep
architectures
include
many
variants
of
a
few
basic
approaches
Each
architecture
has
found
success
in
specific
domains
It
is
not
always
possible
to
compare
the
performance
of
multiple
architectures
unless
they
have
been
evaluated
on
the
same
data
sets
DNNs
are
typically
feedforward
networks
in
which
data
flows
from
the
input
layer
to
the
output
layer
without
looping
back
Recurrent
neural
networks
RNNs
in
which
data
can
flow
in
any
direction
are
used
for
applications
such
as
language
modeling
Long
short
term
memory
is
particularly
effective
for
this
use
Convolutional
deep
neural
networks
CNNs
are
used
in
computer
vision
CNNs
also
have
been
applied
to
acoustic
modeling
for
automatic
speech
recognition
ASR
Challenges
As
with
ANNs
many
issues
can
arise
with
naively
trained
DNNs
Two
common
issues
are
overfitting
and
computation
time
DNNs
are
prone
to
overfitting
because
of
the
added
layers
of
abstraction
which
allow
them
to
model
rare
dependencies
in
the
training
data
Regularization
methods
such
as
Ivakhnenko
s
unit
pruning
or
weight
decay
displaystyle
ell
ell
regularization
or
sparsity
displaystyle
ell
ell
regularization
can
be
applied
during
training
to
combat
overfitting
Alternatively
dropout
regularization
randomly
omits
units
from
the
hidden
layers
during
training
This
helps
to
exclude
rare
dependencies
Finally
data
can
be
augmented
via
methods
such
as
cropping
and
rotating
such
that
smaller
training
sets
can
be
increased
in
size
to
reduce
the
chances
of
overfitting
DNNs
must
consider
many
training
parameters
such
as
the
size
number
of
layers
and
number
of
units
per
layer
the
learning
rate
and
initial
weights
Sweeping
through
the
parameter
space
for
optimal
parameters
may
not
be
feasible
due
to
the
cost
in
time
and
computational
resources
Various
tricks
such
as
batching
computing
the
gradient
on
several
training
examples
at
once
rather
than
individual
examples
speed
up
computation
Large
processing
capabilities
of
many
core
architectures
such
as
GPUs
or
the
Intel
Xeon
Phi
have
produced
significant
speedups
in
training
because
of
the
suitability
of
such
processing
architectures
for
the
matrix
and
vector
computations
Alternatively
engineers
may
look
for
other
types
of
neural
networks
with
more
straightforward
and
convergent
training
algorithms
CMAC
cerebellar
model
articulation
controller
is
one
such
kind
of
neural
network
It
doesn
t
require
learning
rates
or
randomized
initial
weights
for
CMAC
The
training
process
can
be
guaranteed
to
converge
in
one
step
with
a
new
batch
of
data
and
the
computational
complexity
of
the
training
algorithm
is
linear
with
respect
to
the
number
of
neurons
involved
Applications
Automatic
speech
recognition
Main
article
Speech
recognition
Large
scale
automatic
speech
recognition
is
the
first
and
most
convincing
successful
case
of
deep
learning
LSTM
RNNs
can
learn
Very
Deep
Learning
tasks
that
involve
multi
second
intervals
containing
speech
events
separated
by
thousands
of
discrete
time
steps
where
one
time
step
corresponds
to
about
ms
LSTM
with
forget
gates
is
competitive
with
traditional
speech
recognizers
on
certain
tasks
The
initial
success
in
speech
recognition
was
based
on
small
scale
recognition
tasks
based
on
TIMIT
The
data
set
contains
speakers
from
eight
major
dialects
of
American
English
where
each
speaker
reads
sentences
Its
small
size
lets
many
configurations
be
tried
More
importantly
the
TIMIT
task
concerns
phone
sequence
recognition
which
unlike
word
sequence
recognition
allows
weak
language
models
without
a
strong
grammar
clarification
needed
This
lets
the
weaknesses
in
acoustic
modeling
aspects
of
speech
recognition
be
more
easily
analyzed
The
error
rates
listed
below
including
these
early
results
and
measured
as
percent
phone
error
rates
PER
have
been
summarized
over
the
past
years
clarification
needed
Method
PER
Randomly
Initialized
RNN
Bayesian
Triphone
GMM
HMM
Hidden
Trajectory
Generative
Model
Monophone
Randomly
Initialized
DNN
Monophone
DBN
DNN
Triphone
GMM
HMM
with
BMMI
Training
Monophone
DBN
DNN
on
fbank
Convolutional
DNN
Convolutional
DNN
w
Heterogeneous
Pooling
Ensemble
DNN
CNN
RNN
Bidirectional
LSTM
Hierarchical
Convolutional
Deep
Maxout
Network
The
debut
of
DNNs
for
speaker
recognition
in
the
late
s
and
speech
recognition
around
and
of
LSTM
around
accelerated
progress
in
eight
major
areas
Scale
up
out
and
acclerated
DNN
training
and
decoding
Sequence
discriminative
training
Feature
processing
by
deep
models
with
solid
understanding
of
the
underlying
mechanisms
Adaptation
of
DNNs
and
related
deep
models
Multi
task
and
transfer
learning
by
DNNs
and
related
deep
models
CNNs
and
how
to
design
them
to
best
exploit
domain
knowledge
of
speech
RNN
and
its
rich
LSTM
variants
Other
types
of
deep
models
including
tensor
based
models
and
integrated
deep
generative
discriminative
models
All
major
commercial
speech
recognition
systems
e
g
Microsoft
Cortana
Xbox
Skype
Translator
Amazon
Alexa
Google
Now
Apple
Siri
Baidu
and
iFlyTek
voice
search
and
a
range
of
Nuance
speech
products
etc
are
based
on
deep
learning
Image
recognition
Main
article
Computer
vision
A
common
evaluation
set
for
image
classification
is
the
MNIST
database
data
set
MNIST
is
composed
of
handwritten
digits
and
includes
training
examples
and
test
examples
As
with
TIMIT
its
small
size
lets
users
test
multiple
configurations
A
comprehensive
list
of
results
on
this
set
is
available
Deep
learning
based
image
recognition
has
become
superhuman
producing
more
accurate
results
than
human
contestants
This
first
occurred
in
Deep
learning
trained
vehicles
now
interpret
camera
views
Another
example
is
Facial
Dysmorphology
Novel
Analysis
FDNA
used
to
analyze
cases
of
human
malformation
connected
to
a
large
database
of
genetic
syndromes
Visual
art
processing
Closely
related
to
the
progress
that
has
been
made
in
image
recognition
is
the
increasing
application
of
deep
learning
techniques
to
various
visual
art
tasks
DNNs
have
proven
themselves
capable
for
example
of
a
identifying
the
style
period
of
a
given
painting
b
capturing
the
style
of
a
given
painting
and
applying
it
in
a
visually
pleasing
manner
to
an
arbitrary
photograph
and
c
generating
striking
imagery
based
on
random
visual
input
fields
Natural
language
processing
Main
article
Natural
language
processing
Neural
networks
have
been
used
for
implementing
language
models
since
the
early
s
LSTM
helped
to
improve
machine
translation
and
language
modeling
Other
key
techniques
in
this
field
are
negative
sampling
and
word
embedding
Word
embedding
such
as
word
vec
can
be
thought
of
as
a
representational
layer
in
a
deep
learning
architecture
that
transforms
an
atomic
word
into
a
positional
representation
of
the
word
relative
to
other
words
in
the
dataset
the
position
is
represented
as
a
point
in
a
vector
space
Using
word
embedding
as
an
RNN
input
layer
allows
the
network
to
parse
sentences
and
phrases
using
an
effective
compositional
vector
grammar
A
compositional
vector
grammar
can
be
thought
of
as
probabilistic
context
free
grammar
PCFG
implemented
by
an
RNN
Recursive
auto
encoders
built
atop
word
embeddings
can
assess
sentence
similarity
and
detect
paraphrasing
Deep
neural
architectures
provide
the
best
results
for
constituency
parsing
sentiment
analysis
information
retrieval
spoken
language
understanding
machine
translation
contextual
entity
linking
writing
style
recognition
Text
classifcation
and
others
Google
Translate
GT
uses
a
large
end
to
end
long
short
term
memory
network
GNMT
uses
an
example
based
machine
translation
method
in
which
the
system
learns
from
millions
of
examples
It
translates
whole
sentences
at
a
time
rather
than
pieces
Google
Translate
supports
over
one
hundred
languages
The
network
encodes
the
semantics
of
the
sentence
rather
than
simply
memorizing
phrase
to
phrase
translations
GT
uses
English
as
an
intermediate
between
most
language
pairs
Reliability
of
infrastructure
systems
Natural
disasters
can
have
catastrophic
impacts
on
the
functionality
of
infrastructure
systems
and
cause
severe
physical
and
socio
economic
losses
Given
budget
constraints
it
is
crucial
to
optimize
decisions
regarding
mitigation
preparedness
response
and
recovery
practices
for
these
systems
This
requires
accurate
and
efficient
means
to
evaluate
the
infrastructure
system
reliability
Deep
neural
networks
have
been
used
for
accurate
efficient
and
accelerated
infrastructure
system
reliability
analysis
Drug
discovery
and
toxicology
For
more
information
see
Drug
discovery
and
Toxicology
A
large
percentage
of
candidate
drugs
fail
to
win
regulatory
approval
These
failures
are
caused
by
insufficient
efficacy
on
target
effect
undesired
interactions
off
target
effects
or
unanticipated
toxic
effects
Research
has
explored
use
of
deep
learning
to
predict
biomolecular
target
off
target
and
toxic
effects
of
environmental
chemicals
in
nutrients
household
products
and
drugs
AtomNet
is
a
deep
learning
system
for
structure
based
rational
drug
design
AtomNet
was
used
to
predict
novel
candidate
biomolecules
for
disease
targets
such
as
the
Ebola
virus
and
multiple
sclerosis
Customer
relationship
management
Main
article
Customer
relationship
management
Deep
reinforcement
learning
has
been
used
to
approximate
the
value
of
possible
direct
marketing
actions
defined
in
terms
of
RFM
variables
The
estimated
value
function
was
shown
to
have
a
natural
interpretation
as
customer
lifetime
value
Recommendation
systems
Main
article
Recommender
system
Recommendation
systems
have
used
deep
learning
to
extract
meaningful
features
for
a
latent
factor
model
for
content
based
music
recommendations
Multiview
deep
learning
has
been
applied
for
learning
user
preferences
from
multiple
domains
The
model
uses
a
hybrid
collaborative
and
content
based
approach
and
enhances
recommendations
in
multiple
tasks
Bioinformatics
Main
article
Bioinformatics
An
autoencoder
ANN
was
used
in
bioinformatics
to
predict
gene
ontology
annotations
and
gene
function
relationships
In
medical
informatics
deep
learning
was
used
to
predict
sleep
quality
based
on
data
from
wearables
and
predictions
of
health
complications
from
electronic
health
record
data
Deep
learning
has
also
showed
efficacy
in
healthcare
Mobile
advertising
Finding
the
appropriate
mobile
audience
for
mobile
advertising
is
always
challenging
since
many
data
points
must
be
considered
and
assimilated
before
a
target
segment
can
be
created
and
used
in
ad
serving
by
any
ad
server
Deep
learning
has
been
used
to
interpret
large
many
dimensioned
advertising
datasets
Many
data
points
are
collected
during
the
request
serve
click
internet
advertising
cycle
This
information
can
form
the
basis
of
machine
learning
to
improve
ad
selection
Image
restoration
Deep
learning
has
been
successfully
applied
to
inverse
problems
such
as
denoising
super
resolution
and
inpainting
These
applications
include
learning
methods
such
Shrinkage
Fields
for
Effective
Image
Restoration
which
trains
on
an
image
dataset
and
Deep
Image
Prior
which
trains
on
the
image
that
needs
restoration
Relation
to
human
cognitive
and
brain
development
Deep
learning
is
closely
related
to
a
class
of
theories
of
brain
development
specifically
neocortical
development
proposed
by
cognitive
neuroscientists
in
the
early
s
These
developmental
theories
were
instantiated
in
computational
models
making
them
predecessors
of
deep
learning
systems
These
developmental
models
share
the
property
that
various
proposed
learning
dynamics
in
the
brain
e
g
a
wave
of
nerve
growth
factor
support
the
self
organization
somewhat
analogous
to
the
neural
networks
utilized
in
deep
learning
models
Like
the
neocortex
neural
networks
employ
a
hierarchy
of
layered
filters
in
which
each
layer
considers
information
from
a
prior
layer
or
the
operating
environment
and
then
passes
its
output
and
possibly
the
original
input
to
other
layers
This
process
yields
a
self
organizing
stack
of
transducers
well
tuned
to
their
operating
environment
A
description
stated
the
infant
s
brain
seems
to
organize
itself
under
the
influence
of
waves
of
so
called
trophic
factors
different
regions
of
the
brain
become
connected
sequentially
with
one
layer
of
tissue
maturing
before
another
and
so
on
until
the
whole
brain
is
mature
A
variety
of
approaches
have
been
used
to
investigate
the
plausibility
of
deep
learning
models
from
a
neurobiological
perspective
On
the
one
hand
several
variants
of
the
backpropagation
algorithm
have
been
proposed
in
order
to
increase
its
processing
realism
Other
researchers
have
argued
that
unsupervised
forms
of
deep
learning
such
as
those
based
on
hierarchical
generative
models
and
deep
belief
networks
may
be
closer
to
biological
reality
In
this
respect
generative
neural
network
models
have
been
related
to
neurobiological
evidence
about
sampling
based
processing
in
the
cerebral
cortex
Although
a
systematic
comparison
between
the
human
brain
organization
and
the
neuronal
encoding
in
deep
networks
has
not
yet
been
established
several
analogies
have
been
reported
For
example
the
computations
performed
by
deep
learning
units
could
be
similar
to
those
of
actual
neurons
and
neural
populations
Similarly
the
representations
developed
by
deep
learning
models
are
similar
to
those
measured
in
the
primate
visual
system
both
at
the
single
unit
and
at
the
population
levels
Commercial
activity
Many
organizations
employ
deep
learning
for
particular
applications
Facebook
s
AI
lab
performs
tasks
such
as
automatically
tagging
uploaded
pictures
with
the
names
of
the
people
in
them
Google
s
DeepMind
Technologies
developed
a
system
capable
of
learning
how
to
play
Atari
video
games
using
only
pixels
as
data
input
In
they
demonstrated
their
AlphaGo
system
which
learned
the
game
of
Go
well
enough
to
beat
a
professional
Go
player
Google
Translate
uses
an
LSTM
to
translate
between
more
than
languages
In
Blippar
demonstrated
a
mobile
augmented
reality
application
that
uses
deep
learning
to
recognize
objects
in
real
time
Criticism
and
comment
Deep
learning
has
attracted
both
criticism
and
comment
in
some
cases
from
outside
the
field
of
computer
science
Theory
See
also
Explainable
AI
A
main
criticism
concerns
the
lack
of
theory
surrounding
the
methods
citation
needed
Learning
in
the
most
common
deep
architectures
is
implemented
using
well
understood
gradient
descent
However
the
theory
surrounding
other
algorithms
such
as
contrastive
divergence
is
less
clear
citation
needed
e
g
Does
it
converge
If
so
how
fast
What
is
it
approximating
Deep
learning
methods
are
often
looked
at
as
a
black
box
with
most
confirmations
done
empirically
rather
than
theoretically
Others
point
out
that
deep
learning
should
be
looked
at
as
a
step
towards
realizing
strong
AI
not
as
an
all
encompassing
solution
Despite
the
power
of
deep
learning
methods
they
still
lack
much
of
the
functionality
needed
for
realizing
this
goal
entirely
Research
psychologist
Gary
Marcus
noted
Realistically
deep
learning
is
only
part
of
the
larger
challenge
of
building
intelligent
machines
Such
techniques
lack
ways
of
representing
causal
relationships
have
no
obvious
ways
of
performing
logical
inferences
and
they
are
also
still
a
long
way
from
integrating
abstract
knowledge
such
as
information
about
what
objects
are
what
they
are
for
and
how
they
are
typically
used
The
most
powerful
A
I
systems
like
Watson
use
techniques
like
deep
learning
as
just
one
element
in
a
very
complicated
ensemble
of
techniques
ranging
from
the
statistical
technique
of
Bayesian
inference
to
deductive
reasoning
As
an
alternative
to
this
emphasis
on
the
limits
of
deep
learning
one
author
speculated
that
it
might
be
possible
to
train
a
machine
vision
stack
to
perform
the
sophisticated
task
of
discriminating
between
old
master
and
amateur
figure
drawings
and
hypothesized
that
such
a
sensitivity
might
represent
the
rudiments
of
a
non
trivial
machine
empathy
This
same
author
proposed
that
this
would
be
in
line
with
anthropology
which
identifies
a
concern
with
aesthetics
as
a
key
element
of
behavioral
modernity
In
further
reference
to
the
idea
that
artistic
sensitivity
might
inhere
within
relatively
low
levels
of
the
cognitive
hierarchy
a
published
series
of
graphic
representations
of
the
internal
states
of
deep
layers
neural
networks
attempting
to
discern
within
essentially
random
data
the
images
on
which
they
were
trained
demonstrate
a
visual
appeal
the
original
research
notice
received
well
over
comments
and
was
the
subject
of
what
was
for
a
time
the
most
frequently
accessed
article
on
The
Guardian
s
web
site
Errors
Some
deep
learning
architectures
display
problematic
behaviors
such
as
confidently
classifying
unrecognizable
images
as
belonging
to
a
familiar
category
of
ordinary
images
and
misclassifying
minuscule
perturbations
of
correctly
classified
images
Goertzel
hypothesized
that
these
behaviors
are
due
to
limitations
in
their
internal
representations
and
that
these
limitations
would
inhibit
integration
into
heterogeneous
multi
component
AGI
architectures
These
issues
may
possibly
be
addressed
by
deep
learning
architectures
that
internally
form
states
homologous
to
image
grammar
decompositions
of
observed
entities
and
events
Learning
a
grammar
visual
or
linguistic
from
training
data
would
be
equivalent
to
restricting
the
system
to
commonsense
reasoning
that
operates
on
concepts
in
terms
of
grammatical
production
rules
and
is
a
basic
goal
of
both
human
language
acquisition
and
AI
Cyberthreat
As
deep
learning
moves
from
the
lab
into
the
world
research
and
experience
shows
that
artificial
neural
networks
are
vulnerable
to
hacks
and
deception
By
identifying
patterns
that
these
systems
use
to
function
attackers
can
modify
inputs
to
ANNs
in
such
a
way
that
the
ANN
finds
a
match
that
human
observers
would
not
recognize
For
example
an
attacker
can
make
subtle
changes
to
an
image
such
that
the
ANN
finds
a
match
even
though
the
image
looks
to
a
human
nothing
like
the
search
target
Such
a
manipulation
is
termed
an
adversarial
attack
In
researchers
used
one
ANN
to
doctor
images
in
trial
and
error
fashion
identify
another
s
focal
points
and
thereby
generate
images
that
deceived
it
The
modified
images
looked
no
different
to
human
eyes
Another
group
showed
that
printouts
of
doctored
images
then
photographed
successfully
tricked
an
image
classification
system
One
defense
is
reverse
image
search
in
which
a
possible
fake
image
is
submitted
to
a
site
such
as
TinEye
that
can
then
find
other
instances
of
it
A
refinement
is
to
search
using
only
parts
of
the
image
to
identify
images
from
which
that
piece
may
have
been
taken
Another
group
showed
that
certain
psychedelic
spectacles
could
fool
a
facial
recognition
system
into
thinking
ordinary
people
were
celebrities
potentially
allowing
one
person
to
impersonate
another
In
researchers
added
stickers
to
stop
signs
and
caused
an
ANN
to
misclassify
them
ANNs
can
however
be
further
trained
to
detect
attempts
at
deception
potentially
leading
attackers
and
defenders
into
an
arms
race
similar
to
the
kind
that
already
defines
the
malware
defense
industry
ANNs
have
been
trained
to
defeat
ANN
based
anti
malware
software
by
repeatedly
attacking
a
defense
with
malware
that
was
continually
altered
by
a
genetic
algorithm
until
it
tricked
the
anti
malware
while
retaining
its
ability
to
damage
the
target
Another
group
demonstrated
that
certain
sounds
could
make
the
Google
Now
voice
command
system
open
a
particular
web
address
that
would
download
malware
In
data
poisoning
false
data
is
continually
smuggled
into
a
machine
learning
system
s
training
set
to
prevent
it
from
achieving
mastery
See
also
Applications
of
artificial
intelligence
Artificial
neural
networks
Hierarchical
Deep
Learning
Boltzmann
machine
Comparison
of
deep
learning
software
Compressed
sensing
Echo
state
network
List
of
artificial
intelligence
projects
Liquid
state
machine
List
of
datasets
for
machine
learning
research
Reservoir
computing
Sparse
coding
References
Bengio
Y
Courville
A
Vincent
P
Representation
Learning
A
Review
and
New
Perspectives
IEEE
Transactions
on
Pattern
Analysis
and
Machine
Intelligence
C
arXiv
Freely
accessible
doi
tpami
Schmidhuber
J
Deep
Learning
in
Neural
Networks
An
Overview
Neural
Networks
C
arXiv
Freely
accessible
doi
j
neunet
PMID
Bengio
Yoshua
LeCun
Yann
Hinton
Geoffrey
Deep
Learning
Nature
C
doi
nature
PMID
Ghasemi
F
Mehridehnavi
AR
Fassihi
A
Perez
Sanchez
H
Deep
Neural
Network
in
Biological
Activity
Prediction
using
Deep
Belief
Network
Applied
Soft
Computing
doi
j
asoc
Ciresan
Dan
Meier
U
Schmidhuber
J
June
Multi
column
deep
neural
networks
for
image
classification
IEEE
Conference
on
Computer
Vision
and
Pattern
Recognition
C
arXiv
Freely
accessible
doi
cvpr
ISBN
Krizhevsky
Alex
Sutskever
Ilya
Hinton
Geoffry
ImageNet
Classification
with
Deep
Convolutional
Neural
Networks
PDF
NIPS
Neural
Information
Processing
Systems
Lake
Tahoe
Nevada
Marblestone
Adam
H
Wayne
Greg
Kording
Konrad
P
Toward
an
Integration
of
Deep
Learning
and
Neuroscience
Frontiers
in
Computational
Neuroscience
doi
fncom
PMC
Freely
accessible
PMID
Olshausen
B
A
Emergence
of
simple
cell
receptive
field
properties
by
learning
a
sparse
code
for
natural
images
Nature
C
Bibcode
Natur
O
doi
a
PMID
Bengio
Yoshua
Lee
Dong
Hyun
Bornschein
Jorg
Mesnard
Thomas
Lin
Zhouhan
Towards
Biologically
Plausible
Deep
Learning
arXiv
Freely
accessible
cs
LG
Deng
L
Yu
D
Deep
Learning
Methods
and
Applications
PDF
Foundations
and
Trends
in
Signal
Processing
C
C
doi
Bengio
Yoshua
Learning
Deep
Architectures
for
AI
PDF
Foundations
and
Trends
in
Machine
Learning
C
doi
LeCun
Yann
Bengio
Yoshua
Hinton
Geoffrey
May
Deep
learning
Nature
C
doi
nature
J
rgen
Schmidhuber
Deep
Learning
Scholarpedia
Online
Hinton
G
E
Deep
belief
networks
Scholarpedia
doi
scholarpedia
Bal
zs
Csan
d
Cs
ji
Approximation
with
Artificial
Neural
Networks
Faculty
of
Sciences
E
tv
s
Lor
nd
University
Hungary
Cybenko
Approximations
by
superpositions
of
sigmoidal
functions
PDF
Mathematics
of
Control
Signals
and
Systems
C
doi
bf
Archived
from
the
original
PDF
on
Hornik
Kurt
Approximation
Capabilities
of
Multilayer
Feedforward
Networks
Neural
Networks
C
doi
t
Haykin
Simon
S
Neural
Networks
A
Comprehensive
Foundation
Prentice
Hall
ISBN
Hassoun
Mohamad
H
Fundamentals
of
Artificial
Neural
Networks
MIT
Press
p
ISBN
Murphy
Kevin
P
August
Machine
Learning
A
Probabilistic
Perspective
MIT
Press
ISBN
Patel
Ankit
Nguyen
Tan
Baraniuk
Richard
A
Probabilistic
Framework
for
Deep
Learning
PDF
Advances
in
Neural
Information
Processing
Systems
Hinton
G
E
Srivastava
N
Krizhevsky
A
Sutskever
I
Salakhutdinov
R
R
Improving
neural
networks
by
preventing
co
adaptation
of
feature
detectors
arXiv
Freely
accessible
math
LG
Bishop
Christopher
M
Pattern
Recognition
and
Machine
Learning
PDF
Springer
ISBN
Rina
Dechter
Learning
while
searching
in
constraint
satisfaction
problems
University
of
California
Computer
Science
Department
Cognitive
Systems
Laboratory
Online
Igor
Aizenberg
Naum
N
Aizenberg
Joos
P
L
Vandewalle
Multi
Valued
and
Universal
Binary
Neurons
Theory
Learning
and
Applications
Springer
Science
Business
Media
Co
evolving
recurrent
neurons
learn
deep
memory
POMDPs
Proc
GECCO
Washington
D
C
pp
ACM
Press
New
York
NY
USA
Ivakhnenko
A
G
Cybernetic
Predicting
Devices
CCM
Information
Corporation
Ivakhnenko
Alexey
Polynomial
theory
of
complex
systems
IEEE
Transactions
on
Systems
Man
and
Cybernetics
C
doi
TSMC
Fukushima
K
Neocognitron
A
self
organizing
neural
network
model
for
a
mechanism
of
pattern
recognition
unaffected
by
shift
in
position
Biol
Cybern
C
doi
bf
PMID
Seppo
Linnainmaa
The
representation
of
the
cumulative
rounding
error
of
an
algorithm
as
a
Taylor
expansion
of
the
local
rounding
errors
Master
s
Thesis
in
Finnish
Univ
Helsinki
Griewank
Andreas
Who
Invented
the
Reverse
Mode
of
Differentiation
PDF
Documenta
Matematica
Extra
Volume
ISMP
C
Werbos
P
Beyond
Regression
New
Tools
for
Prediction
and
Analysis
in
the
Behavioral
Sciences
Harvard
University
Retrieved
June
Werbos
Paul
Applications
of
advances
in
nonlinear
sensitivity
analysis
System
modeling
and
optimization
PDF
Springer
pp
C
LeCun
et
al
Backpropagation
Applied
to
Handwritten
Zip
Code
Recognition
Neural
Computation
pp
C
J
Weng
N
Ahuja
and
T
S
Huang
Cresceptron
a
self
organizing
neural
network
which
grows
adaptively
Proc
International
Joint
Conference
on
Neural
Networks
Baltimore
Maryland
vol
I
pp
June
J
Weng
N
Ahuja
and
T
S
Huang
Learning
recognition
and
segmentation
of
D
objects
from
D
images
Proc
th
International
Conf
Computer
Vision
Berlin
Germany
pp
May
J
Weng
N
Ahuja
and
T
S
Huang
Learning
recognition
and
segmentation
using
the
Cresceptron
International
Journal
of
Computer
Vision
vol
no
pp
Nov
de
Carvalho
Andre
C
L
F
Fairhurst
Mike
C
Bisset
David
An
integrated
Boolean
neural
network
for
pattern
classification
Pattern
Recognition
Letters
C
doi
Hinton
Geoffrey
E
Dayan
Peter
Frey
Brendan
J
Neal
Radford
The
wake
sleep
algorithm
for
unsupervised
neural
networks
Science
C
doi
science
S
Hochreiter
Untersuchungen
zu
dynamischen
neuronalen
Netzen
Diploma
thesis
Institut
f
Informatik
Technische
Univ
Munich
Advisor
J
Schmidhuber
Hochreiter
S
et
al
January
Gradient
flow
in
recurrent
nets
the
difficulty
of
learning
long
term
dependencies
In
Kolen
John
F
Kremer
Stefan
C
A
Field
Guide
to
Dynamical
Recurrent
Networks
John
Wiley
Sons
ISBN
Morgan
Nelson
Bourlard
Herv
Renals
Steve
Cohen
Michael
Franco
Horacio
Hybrid
neural
network
hidden
markov
model
systems
for
continuous
speech
recognition
International
Journal
of
Pattern
Recognition
and
Artificial
Intelligence
C
doi
s
ISSN
Robinson
T
A
real
time
recurrent
error
propagation
network
word
recognition
system
ICASSP
Waibel
A
Hanazawa
T
Hinton
G
Shikano
K
Lang
K
J
March
Phoneme
recognition
using
time
delay
neural
networks
IEEE
Transactions
on
Acoustics
Speech
and
Signal
Processing
C
doi
ISSN
Baker
J
Deng
Li
Glass
Jim
Khudanpur
S
Lee
C
H
Morgan
N
O
Shaughnessy
D
Research
Developments
and
Directions
in
Speech
Recognition
and
Understanding
Part
IEEE
Signal
Processing
Magazine
C
doi
msp
Bengio
Y
Artificial
Neural
Networks
and
their
Application
to
Speech
Sequence
Recognition
McGill
University
Ph
D
thesis
Deng
L
Hassanein
K
Elmasry
M
Analysis
of
correlation
structure
for
a
neural
predictive
model
with
applications
to
speech
recognition
Neural
Networks
C
doi
Heck
L
Konig
Y
Sonmez
M
Weintraub
M
Robustness
to
Telephone
Handset
Distortion
in
Speaker
Recognition
by
Discriminative
Feature
Design
Speech
Communication
C
doi
s
Acoustic
Modeling
with
Deep
Neural
Networks
Using
Raw
Time
Signal
for
LVCSR
PDF
Download
Available
ResearchGate
Retrieved
Hochreiter
Sepp
Schmidhuber
J
rgen
Long
Short
Term
Memory
Neural
Computation
C
doi
neco
ISSN
PMID
Graves
Alex
Eck
Douglas
Beringer
Nicole
Schmidhuber
J
rgen
Biologically
Plausible
Speech
Recognition
with
LSTM
Neural
Nets
PDF
st
Intl
Workshop
on
Biologically
Inspired
Approaches
to
Advanced
Information
Technology
Bio
ADIT
Lausanne
Switzerland
pp
C
Graves
Alex
Fern
ndez
Santiago
Gomez
Faustino
Connectionist
temporal
classification
Labelling
unsegmented
sequence
data
with
recurrent
neural
networks
In
Proceedings
of
the
International
Conference
on
Machine
Learning
ICML
C
Santiago
Fernandez
Alex
Graves
and
J
rgen
Schmidhuber
An
application
of
recurrent
neural
networks
to
discriminative
keyword
spotting
Proceedings
of
ICANN
pp
C
Sak
Ha
im
Senior
Andrew
Rao
Kanishka
Beaufays
Fran
oise
Schalkwyk
Johan
September
Google
voice
search
faster
and
more
accurate
Hinton
Geoffrey
E
Learning
multiple
layers
of
representation
Trends
in
Cognitive
Sciences
C
doi
j
tics
ISSN
PMID
Hinton
G
E
Osindero
S
Teh
Y
W
A
Fast
Learning
Algorithm
for
Deep
Belief
Nets
PDF
Neural
Computation
C
doi
neco
PMID
Bengio
Yoshua
Practical
recommendations
for
gradient
based
training
of
deep
architectures
arXiv
Freely
accessible
cs
LG
G
E
Hinton
Learning
multiple
layers
of
representation
Trends
in
Cognitive
Sciences
pp
C
Hinton
G
Deng
L
Yu
D
Dahl
G
Mohamed
A
Jaitly
N
Senior
A
Vanhoucke
V
Nguyen
P
Sainath
T
Kingsbury
B
Deep
Neural
Networks
for
Acoustic
Modeling
in
Speech
Recognition
The
shared
views
of
four
research
groups
IEEE
Signal
Processing
Magazine
C
doi
msp
Deng
Li
Hinton
Geoffrey
Kingsbury
Brian
May
New
types
of
deep
neural
network
learning
for
speech
recognition
and
related
applications
An
overview
C
via
research
microsoft
com
Deng
L
Li
J
Huang
J
T
Yao
K
Yu
D
Seide
F
Seltzer
M
Zweig
G
He
X
May
Recent
advances
in
deep
learning
for
speech
research
at
Microsoft
IEEE
International
Conference
on
Acoustics
Speech
and
Signal
Processing
C
doi
icassp
ISBN
Sak
Hasim
Senior
Andrew
Beaufays
Francoise
Long
Short
Term
Memory
recurrent
neural
network
architectures
for
large
scale
acoustic
modeling
PDF
Li
Xiangang
Wu
Xihong
Constructing
Long
Short
Term
Memory
based
Deep
Recurrent
Neural
Networks
for
Large
Vocabulary
Speech
Recognition
arXiv
Freely
accessible
cs
CL
line
feed
character
in
title
at
position
help
Zen
Heiga
Sak
Hasim
Unidirectional
Long
Short
Term
Memory
Recurrent
Neural
Network
with
Recurrent
Output
Layer
for
Low
Latency
Speech
Synthesis
PDF
Google
com
ICASSP
pp
C
Deng
L
Abdel
Hamid
O
Yu
D
A
deep
convolutional
neural
network
using
heterogeneous
pooling
for
trading
acoustic
invariance
with
phonetic
confusion
PDF
Google
com
ICASSP
Sainath
T
N
Mohamed
A
r
Kingsbury
B
Ramabhadran
B
May
Deep
convolutional
neural
networks
for
LVCSR
IEEE
International
Conference
on
Acoustics
Speech
and
Signal
Processing
C
doi
icassp
ISBN
Yann
LeCun
Slides
on
Deep
Learning
Online
NIPS
Workshop
Deep
Learning
for
Speech
Recognition
and
Related
Applications
Whistler
BC
Canada
Dec
Organizers
Li
Deng
Geoff
Hinton
D
Yu
Keynote
talk
Recent
Developments
in
Deep
Neural
Networks
ICASSP
by
Geoff
Hinton
D
Yu
L
Deng
G
Li
and
F
Seide
Discriminative
pretraining
of
deep
neural
networks
U
S
Patent
Filing
Deng
L
Hinton
G
Kingsbury
B
New
types
of
deep
neural
network
learning
for
speech
recognition
and
related
applications
An
overview
ICASSP
PDF
Yu
D
Deng
L
Automatic
Speech
Recognition
A
Deep
Learning
Approach
Publisher
Springer
ISBN
Deng
receives
prestigious
IEEE
Technical
Achievement
Award
Microsoft
Research
Microsoft
Research
December
Li
Deng
September
Keynote
talk
Achievements
and
Challenges
of
Deep
Learning
From
Speech
Analysis
and
Recognition
To
Language
and
Multimodal
Processing
Interspeech
Yu
D
Deng
L
Roles
of
Pre
Training
and
Fine
Tuning
in
Context
Dependent
DBN
HMMs
for
Real
World
Speech
Recognition
NIPS
Workshop
on
Deep
Learning
and
Unsupervised
Feature
Learning
Seide
F
Li
G
Yu
D
Conversational
speech
transcription
using
context
dependent
deep
neural
networks
Interspeech
Deng
Li
Li
Jinyu
Huang
Jui
Ting
Yao
Kaisheng
Yu
Dong
Seide
Frank
Seltzer
Mike
Zweig
Geoff
He
Xiaodong
Recent
Advances
in
Deep
Learning
for
Speech
Research
at
Microsoft
Microsoft
Research
Nvidia
CEO
bets
big
on
deep
learning
and
VR
Venture
Beat
April
From
not
working
to
neural
networking
The
Economist
Oh
K
S
Jung
K
GPU
implementation
of
neural
networks
Pattern
Recognition
C
doi
j
patcog
Chellapilla
K
Puri
S
and
Simard
P
High
performance
convolutional
neural
networks
for
document
processing
International
Workshop
on
Frontiers
in
Handwriting
Recognition
Cire
an
Dan
Claudiu
Meier
Ueli
Gambardella
Luca
Maria
Schmidhuber
J
rgen
Deep
Big
Simple
Neural
Nets
for
Handwritten
Digit
Recognition
Neural
Computation
C
doi
neco
a
ISSN
Raina
Rajat
Madhavan
Anand
Ng
Andrew
Y
Large
scale
Deep
Unsupervised
Learning
Using
Graphics
Processors
Proceedings
of
the
th
Annual
International
Conference
on
Machine
Learning
ICML
New
York
NY
USA
ACM
C
CiteSeerX
Freely
accessible
doi
ISBN
Sze
Vivienne
Chen
Yu
Hsin
Yang
Tien
Ju
Emer
Joel
Efficient
Processing
of
Deep
Neural
Networks
A
Tutorial
and
Survey
arXiv
Freely
accessible
cs
CV
Announcement
of
the
winners
of
the
Merck
Molecular
Activity
Challenge
Multi
task
Neural
Networks
for
QSAR
Predictions
Data
Science
Association
www
datascienceassn
org
Retrieved
Toxicology
in
the
st
century
Data
Challenge
NCATS
Announces
Tox
Data
Challenge
Winners
Archived
copy
Archived
from
the
original
on
Retrieved
Ciresan
D
C
Meier
U
Masci
J
Gambardella
L
M
Schmidhuber
J
Flexible
High
Performance
Convolutional
Neural
Networks
for
Image
Classification
PDF
International
Joint
Conference
on
Artificial
Intelligence
doi
ijcai
Ciresan
Dan
Giusti
Alessandro
Gambardella
Luca
M
Schmidhuber
Juergen
Pereira
F
Burges
C
J
C
Bottou
L
Weinberger
K
Q
eds
Advances
in
Neural
Information
Processing
Systems
PDF
Curran
Associates
Inc
pp
C
Ciresan
D
Giusti
A
Gambardella
L
M
Schmidhuber
J
Mitosis
Detection
in
Breast
Cancer
Histology
Images
using
Deep
Neural
Networks
PDF
Proceedings
MICCAI
The
Wolfram
Language
Image
Identification
Project
www
imageidentify
com
Retrieved
Vinyals
Oriol
Toshev
Alexander
Bengio
Samy
Erhan
Dumitru
Show
and
Tell
A
Neural
Image
Caption
Generator
arXiv
Freely
accessible
cs
CV
Fang
Hao
Gupta
Saurabh
Iandola
Forrest
Srivastava
Rupesh
Deng
Li
Doll
r
Piotr
Gao
Jianfeng
He
Xiaodong
Mitchell
Margaret
Platt
John
C
Lawrence
Zitnick
C
Zweig
Geoffrey
From
Captions
to
Visual
Concepts
and
Back
arXiv
Freely
accessible
cs
CV
Kiros
Ryan
Salakhutdinov
Ruslan
Zemel
Richard
S
Unifying
Visual
Semantic
Embeddings
with
Multimodal
Neural
Language
Models
arXiv
Freely
accessible
cs
LG
line
feed
character
in
title
at
position
help
Zhong
Sheng
hua
Liu
Yan
Liu
Yang
Bilinear
Deep
Learning
for
Image
Classification
Proceedings
of
the
th
ACM
International
Conference
on
Multimedia
MM
New
York
NY
USA
ACM
C
doi
ISBN
Kowsari
Kamran
Heidarysafa
Mojtaba
Brown
Donald
E
Meimandi
Kiana
Jafari
Barnes
Laura
E
RMDL
Random
Multimodel
Deep
Learning
for
Classification
arXiv
Freely
accessible
cs
LG
Why
Deep
Learning
Is
Suddenly
Changing
Your
Life
Fortune
Retrieved
April
Silver
David
Huang
Aja
Maddison
Chris
J
Guez
Arthur
Sifre
Laurent
Driessche
George
van
den
Schrittwieser
Julian
Antonoglou
Ioannis
Panneershelvam
Veda
January
Mastering
the
game
of
Go
with
deep
neural
networks
and
tree
search
Nature
C
doi
nature
ISSN
PMID
Szegedy
Christian
Toshev
Alexander
Erhan
Dumitru
Deep
neural
networks
for
object
detection
Advances
in
Neural
Information
Processing
Systems
Gers
Felix
A
Schmidhuber
J
rgen
LSTM
Recurrent
Networks
Learn
Simple
Context
Free
and
Context
Sensitive
Languages
IEEE
Trans
Neural
Netw
C
doi
Sutskever
L
Vinyals
O
Le
Q
Sequence
to
Sequence
Learning
with
Neural
Networks
PDF
Proc
NIPS
Jozefowicz
Rafal
Vinyals
Oriol
Schuster
Mike
Shazeer
Noam
Wu
Yonghui
Exploring
the
Limits
of
Language
Modeling
arXiv
Freely
accessible
cs
CL
Gillick
Dan
Brunk
Cliff
Vinyals
Oriol
Subramanya
Amarnag
Multilingual
Language
Processing
from
Bytes
arXiv
Freely
accessible
cs
CL
Mikolov
T
et
al
Recurrent
neural
network
based
language
model
PDF
Interspeech
Learning
Precise
Timing
with
LSTM
Recurrent
Networks
PDF
Download
Available
ResearchGate
Retrieved
LeCun
Y
et
al
Gradient
based
learning
applied
to
document
recognition
Proceedings
of
the
IEEE
C
doi
Bengio
Y
Boulanger
Lewandowski
N
Pascanu
R
May
Advances
in
optimizing
recurrent
networks
IEEE
International
Conference
on
Acoustics
Speech
and
Signal
Processing
C
arXiv
Freely
accessible
doi
icassp
ISBN
Dahl
G
et
al
Improving
DNNs
for
LVCSR
using
rectified
linear
units
and
dropout
PDF
ICASSP
Data
Augmentation
deeplearning
ai
Coursera
Coursera
Retrieved
Hinton
G
E
A
Practical
Guide
to
Training
Restricted
Boltzmann
Machines
Tech
Rep
UTML
TR
You
Yang
Bulu
Ayd
n
Demmel
James
November
Scaling
deep
learning
on
GPU
and
knights
landing
clusters
SC
ACM
Retrieved
March
Viebke
Andr
Memeti
Suejb
Pllana
Sabri
Abraham
Ajith
March
CHAOS
a
parallelization
scheme
for
training
convolutional
neural
networks
on
Intel
Xeon
Phi
The
Journal
of
Supercomputing
C
doi
s
x
Ting
Qin
et
al
A
learning
algorithm
of
CMAC
based
on
RLS
Neural
Processing
Letters
Ting
Qin
et
al
Continuous
CMAC
QRLS
and
its
systolic
array
Neural
Processing
Letters
TIMIT
Acoustic
Phonetic
Continuous
Speech
Corpus
Linguistic
Data
Consortium
Philadelphia
Abdel
Hamid
O
et
al
Convolutional
Neural
Networks
for
Speech
Recognition
PDF
IEEE
ACM
Transactions
on
Audio
Speech
and
Language
Processing
C
doi
taslp
Deng
L
Platt
J
Ensemble
Deep
Learning
for
Speech
Recognition
PDF
Proc
Interspeech
T
th
Laszl
Phone
Recognition
with
Hierarchical
Convolutional
Deep
Maxout
Networks
EURASIP
Journal
on
Audio
Speech
and
Music
Processing
doi
s
How
Skype
Used
AI
to
Build
Its
Amazing
New
Language
Translator
WIRED
www
wired
com
Retrieved
Hannun
Awni
Case
Carl
Casper
Jared
Catanzaro
Bryan
Diamos
Greg
Elsen
Erich
Prenger
Ryan
Satheesh
Sanjeev
Sengupta
Shubho
Coates
Adam
Ng
Andrew
Y
Deep
Speech
Scaling
up
end
to
end
speech
recognition
arXiv
Freely
accessible
cs
CL
Plenary
presentation
at
ICASSP
PDF
MNIST
handwritten
digit
database
Yann
LeCun
Corinna
Cortes
and
Chris
Burges
yann
lecun
com
Cire
an
Dan
Meier
Ueli
Masci
Jonathan
Schmidhuber
J
rgen
August
Multi
column
deep
neural
network
for
traffic
sign
classification
Neural
Networks
Selected
Papers
from
IJCNN
C
doi
j
neunet
Nvidia
Demos
a
Car
Computer
Trained
with
Deep
Learning
David
Talbot
MIT
Technology
Review
G
W
Smith
Frederic
Fol
Leymarie
April
The
Machine
as
Artist
An
Introduction
Arts
Retrieved
October
Blaise
Ag
era
y
Arcas
September
Art
in
the
Age
of
Machine
Intelligence
Arts
Retrieved
October
Bengio
Yoshua
Ducharme
R
jean
Vincent
Pascal
Janvin
Christian
March
A
Neural
Probabilistic
Language
Model
J
Mach
Learn
Res
C
ISSN
Goldberg
Yoav
Levy
Omar
word
vec
Explained
Deriving
Mikolov
et
al
s
Negative
Sampling
Word
Embedding
Method
arXiv
Freely
accessible
cs
CL
Socher
Richard
Manning
Christopher
Deep
Learning
for
NLP
PDF
Retrieved
October
Socher
Richard
Bauer
John
Manning
Christopher
Ng
Andrew
Parsing
With
Compositional
Vector
Grammars
PDF
Proceedings
of
the
ACL
conference
Socher
Richard
Recursive
Deep
Models
for
Semantic
Compositionality
Over
a
Sentiment
Treebank
PDF
Shen
Yelong
He
Xiaodong
Gao
Jianfeng
Deng
Li
Mesnil
Gregoire
A
Latent
Semantic
Model
with
Convolutional
Pooling
Structure
for
Information
Retrieval
Microsoft
Research
Huang
Po
Sen
He
Xiaodong
Gao
Jianfeng
Deng
Li
Acero
Alex
Heck
Larry
Learning
Deep
Structured
Semantic
Models
for
Web
Search
using
Clickthrough
Data
Microsoft
Research
Mesnil
G
Dauphin
Y
Yao
K
Bengio
Y
Deng
L
Hakkani
Tur
D
He
X
Heck
L
Tur
G
Yu
D
Zweig
G
Using
recurrent
neural
networks
for
slot
filling
in
spoken
language
understanding
IEEE
Transactions
on
Audio
Speech
and
Language
Processing
C
doi
taslp
Gao
Jianfeng
He
Xiaodong
Yih
Scott
Wen
tau
Deng
Li
Learning
Continuous
Phrase
Representations
for
Translation
Modeling
Microsoft
Research
Brocardo
ML
Traore
I
Woungang
I
Obaidat
MS
Authorship
verification
using
deep
belief
network
systems
Int
J
Commun
Syst
doi
dac
Deep
Learning
for
Natural
Language
Processing
Theory
and
Practice
CIKM
Tutorial
Microsoft
Research
Microsoft
Research
Retrieved
Turovsky
Barak
November
Found
in
translation
More
accurate
fluent
sentences
in
Google
Translate
The
Keyword
Google
Blog
Google
Retrieved
March
Schuster
Mike
Johnson
Melvin
Thorat
Nikhil
November
Zero
Shot
Translation
with
Google
s
Multilingual
Neural
Machine
Translation
System
Google
Research
Blog
Google
Retrieved
March
Sepp
Hochreiter
J
rgen
Schmidhuber
Long
short
term
memory
Neural
Computation
C
doi
neco
PMID
Felix
A
Gers
J
rgen
Schmidhuber
Fred
Cummins
Learning
to
Forget
Continual
Prediction
with
LSTM
Neural
Computation
C
doi
Wu
Yonghui
Schuster
Mike
Chen
Zhifeng
Le
Quoc
V
Norouzi
Mohammad
Macherey
Wolfgang
Krikun
Maxim
Cao
Yuan
Gao
Qin
Macherey
Klaus
Klingner
Jeff
Shah
Apurva
Johnson
Melvin
Liu
Xiaobing
Kaiser
ukasz
Gouws
Stephan
Kato
Yoshikiyo
Kudo
Taku
Kazawa
Hideto
Stevens
Keith
Kurian
George
Patil
Nishant
Wang
Wei
Young
Cliff
Smith
Jason
Riesa
Jason
Rudnick
Alex
Vinyals
Oriol
Corrado
Greg
et
al
Google
s
Neural
Machine
Translation
System
Bridging
the
Gap
between
Human
and
Machine
Translation
arXiv
Freely
accessible
cs
CL
An
Infusion
of
AI
Makes
Google
Translate
More
Powerful
Than
Ever
Cade
Metz
WIRED
Date
of
Publication
https
www
wired
com
google
claims
ai
breakthrough
machine
translation
Boitet
Christian
Blanchon
Herv
Seligman
Mark
Bellynck
Val
rie
MT
on
and
for
the
Web
PDF
Retrieved
December
Nabian
Mohammad
Amin
Meidani
Hadi
Deep
Learning
for
Accelerated
Reliability
Analysis
of
Infrastructure
Networks
arXiv
Freely
accessible
cs
CE
Nabian
Mohammad
Amin
Meidani
Hadi
Accelerating
Stochastic
Assessment
of
Post
Earthquake
Transportation
Network
Connectivity
via
Machine
Learning
Based
Surrogates
Arrowsmith
J
Miller
P
Trial
watch
Phase
II
and
phase
III
attrition
rates
Nature
Reviews
Drug
Discovery
doi
nrd
PMID
Verbist
B
Klambauer
G
Vervoort
L
Talloen
W
The
Qstar
Consortium
Shkedy
Z
Thas
O
Bender
A
G
hlmann
H
W
Hochreiter
S
Using
transcriptomics
to
guide
lead
optimization
in
drug
discovery
projects
Lessons
learned
from
the
QSTAR
project
Drug
Discovery
Today
C
doi
j
drudis
PMID
Wallach
Izhar
Dzamba
Michael
Heifets
Abraham
AtomNet
A
Deep
Convolutional
Neural
Network
for
Bioactivity
Prediction
in
Structure
based
Drug
Discovery
arXiv
Freely
accessible
cs
LG
Toronto
startup
has
a
faster
way
to
discover
effective
medicines
The
Globe
and
Mail
Retrieved
Startup
Harnesses
Supercomputers
to
Seek
Cures
KQED
Future
of
You
Retrieved
Toronto
startup
has
a
faster
way
to
discover
effective
medicines
Tkachenko
Yegor
April
Autonomous
CRM
Control
via
CLV
Approximation
with
Deep
Reinforcement
Learning
in
Discrete
and
Continuous
Action
Space
arXiv
Freely
accessible
cs
LG
van
den
Oord
Aaron
Dieleman
Sander
Schrauwen
Benjamin
Burges
C
J
C
Bottou
L
Welling
M
Ghahramani
Z
Weinberger
K
Q
eds
Advances
in
Neural
Information
Processing
Systems
PDF
Curran
Associates
Inc
pp
C
Elkahky
Ali
Mamdouh
Song
Yang
He
Xiaodong
A
Multi
View
Deep
Learning
Approach
for
Cross
Domain
User
Modeling
in
Recommendation
Systems
Microsoft
Research
Chicco
Davide
Sadowski
Peter
Baldi
Pierre
January
Deep
Autoencoder
Neural
Networks
for
Gene
Ontology
Annotation
Predictions
Proceedings
of
the
th
ACM
Conference
on
Bioinformatics
Computational
Biology
and
Health
Informatics
BCB
ACM
C
doi
ISBN
C
via
ACM
Digital
Library
Sathyanarayana
Aarti
Sleep
Quality
Prediction
From
Wearable
Data
Using
Deep
Learning
JMIR
mHealth
and
uHealth
e
doi
mhealth
PMC
Freely
accessible
PMID
Movahedi
F
Coyle
J
L
Sejdi
E
Deep
belief
networks
for
electroencephalography
A
review
of
recent
contributions
and
future
outlooks
IEEE
Journal
of
Biomedical
and
Health
Informatics
PP
C
doi
JBHI
ISSN
Choi
Edward
Schuetz
Andy
Stewart
Walter
F
Sun
Jimeng
Using
recurrent
neural
network
models
for
early
detection
of
heart
failure
onset
Journal
of
the
American
Medical
Informatics
Association
C
doi
jamia
ocw
ISSN
PMC
Freely
accessible
PMID
Startups
Requests
for
Deep
Learning
in
Healthcare
Challenges
and
Opportunities
Medium
Retrieved
Dudik
Joshua
M
Coyle
James
L
El
Jaroudi
Amro
Mao
Zhi
Hong
Sun
Mingui
Sejdi
Ervin
Deep
learning
for
classification
of
normal
swallows
in
adults
Neurocomputing
C
doi
j
neucom
ISSN
PMC
Freely
accessible
PMID
Using
Deep
Learning
Neural
Networks
To
Find
Best
Performing
Audience
Segments
PDF
IJSTR
De
Shaunak
Maity
Abhishek
Goel
Vritti
Shitole
Sanjay
Bhattacharya
Avik
Predicting
the
popularity
of
instagram
posts
for
a
lifestyle
magazine
using
deep
learning
nd
IEEE
Conference
on
Communication
Systems
Computing
and
IT
Applications
C
doi
CSCITA
ISBN
Schmidt
Uwe
Roth
Stefan
Shrinkage
Fields
for
Effective
Image
Restoration
PDF
Computer
Vision
and
Pattern
Recognition
CVPR
IEEE
Conference
on
Utgoff
P
E
Stracuzzi
D
J
Many
layered
learning
Neural
Computation
C
doi
Elman
Jeffrey
L
Rethinking
Innateness
A
Connectionist
Perspective
on
Development
MIT
Press
ISBN
Shrager
J
Johnson
MH
Dynamic
plasticity
influences
the
emergence
of
function
in
a
simple
cortical
array
Neural
Networks
C
doi
PMID
Quartz
SR
Sejnowski
TJ
The
neural
basis
of
cognitive
development
A
constructivist
manifesto
Behavioral
and
Brain
Sciences
C
CiteSeerX
Freely
accessible
doi
s
x
S
Blakeslee
In
brain
s
early
growth
timetable
may
be
critical
The
New
York
Times
Science
Section
pp
B
CB
Mazzoni
P
Andersen
R
A
Jordan
M
I
A
more
biologically
plausible
learning
rule
for
neural
networks
Proceedings
of
the
National
Academy
of
Sciences
C
doi
pnas
ISSN
PMC
Freely
accessible
PMID
O
Reilly
Randall
C
Biologically
Plausible
Error
Driven
Learning
Using
Local
Activation
Differences
The
Generalized
Recirculation
Algorithm
Neural
Computation
C
doi
neco
ISSN
Testolin
Alberto
Zorzi
Marco
Probabilistic
Models
and
Generative
Neural
Networks
Towards
an
Unified
Framework
for
Modeling
Normal
and
Impaired
Neurocognitive
Functions
Frontiers
in
Computational
Neuroscience
doi
fncom
ISSN
Testolin
Alberto
Stoianov
Ivilin
Zorzi
Marco
September
Letter
perception
emerges
from
unsupervised
deep
learning
and
recycling
of
natural
image
features
Nature
Human
Behaviour
C
doi
s
ISSN
Buesing
Lars
Bill
Johannes
Nessler
Bernhard
Maass
Wolfgang
Neural
Dynamics
as
Sampling
A
Model
for
Stochastic
Computation
in
Recurrent
Networks
of
Spiking
Neurons
PLOS
Computational
Biology
e
doi
journal
pcbi
ISSN
PMID
Morel
Danielle
Singh
Chandan
Levy
William
B
Linearization
of
excitatory
synaptic
integration
at
no
extra
cost
Journal
of
Computational
Neuroscience
C
doi
s
ISSN
Cash
S
Yuste
R
February
Linear
summation
of
excitatory
inputs
by
CA
pyramidal
neurons
Neuron
C
doi
s
ISSN
PMID
Olshausen
B
Field
D
Sparse
coding
of
sensory
inputs
Current
Opinion
in
Neurobiology
C
doi
j
conb
ISSN
Yamins
Daniel
L
K
DiCarlo
James
J
March
Using
goal
driven
deep
learning
models
to
understand
sensory
cortex
Nature
Neuroscience
C
doi
nn
ISSN
Zorzi
Marco
Testolin
Alberto
An
emergentist
perspective
on
the
origin
of
number
sense
Phil
Trans
R
Soc
B
doi
rstb
ISSN
PMC
Freely
accessible
PMID
G
l
Umut
van
Gerven
Marcel
A
J
Deep
Neural
Networks
Reveal
a
Gradient
in
the
Complexity
of
Neural
Representations
across
the
Ventral
Stream
Journal
of
Neuroscience
C
doi
jneurosci
Metz
C
December
Facebook
s
Deep
Learning
Guru
Reveals
the
Future
of
AI
Wired
Google
AI
algorithm
masters
ancient
game
of
Go
Nature
News
Comment
Retrieved
Silver
David
Huang
Aja
Maddison
Chris
J
Guez
Arthur
Sifre
Laurent
Driessche
George
van
den
Schrittwieser
Julian
Antonoglou
Ioannis
Panneershelvam
Veda
Lanctot
Marc
Dieleman
Sander
Grewe
Dominik
Nham
John
Kalchbrenner
Nal
Sutskever
Ilya
Lillicrap
Timothy
Leach
Madeleine
Kavukcuoglu
Koray
Graepel
Thore
Hassabis
Demis
January
Mastering
the
game
of
Go
with
deep
neural
networks
and
tree
search
Nature
C
Bibcode
Natur
S
doi
nature
ISSN
PMID
Retrieved
December
closed
access
publication
C
behind
paywall
A
Google
DeepMind
Algorithm
Uses
Deep
Learning
and
More
to
Master
the
Game
of
Go
MIT
Technology
Review
MIT
Technology
Review
Retrieved
Blippar
Demonstrates
New
Real
Time
Augmented
Reality
App
TechCrunch
Knight
Will
DARPA
is
funding
projects
that
will
try
to
open
up
AI
s
black
boxes
MIT
Technology
Review
Retrieved
Marcus
Gary
November
Is
Deep
Learning
a
Revolution
in
Artificial
Intelligence
The
New
Yorker
Retrieved
Smith
G
W
March
Art
and
Artificial
Intelligence
ArtEnt
Archived
from
the
original
on
June
Retrieved
March
Mellars
Paul
February
The
Impossible
Coincidence
A
Single
Species
Model
for
the
Origins
of
Modern
Human
Behavior
in
Europe
PDF
Evolutionary
Anthropology
Issues
News
and
Reviews
Retrieved
April
Alexander
Mordvintsev
Christopher
Olah
Mike
Tyka
June
Inceptionism
Going
Deeper
into
Neural
Networks
Google
Research
Blog
Retrieved
June
Alex
Hern
June
Yes
androids
do
dream
of
electric
sheep
The
Guardian
Retrieved
June
Goertzel
Ben
Are
there
Deep
Reasons
Underlying
the
Pathologies
of
Today
s
Deep
Learning
Algorithms
PDF
Nguyen
Anh
Yosinski
Jason
Clune
Jeff
Deep
Neural
Networks
are
Easily
Fooled
High
Confidence
Predictions
for
Unrecognizable
Images
arXiv
Freely
accessible
cs
CV
line
feed
character
in
title
at
position
help
Szegedy
Christian
Zaremba
Wojciech
Sutskever
Ilya
Bruna
Joan
Erhan
Dumitru
Goodfellow
Ian
Fergus
Rob
Intriguing
properties
of
neural
networks
arXiv
Freely
accessible
cs
CV
Zhu
S
C
Mumford
D
A
stochastic
grammar
of
images
Found
Trends
Comput
Graph
Vis
C
doi
Miller
G
A
and
N
Chomsky
Pattern
conception
Paper
for
Conference
on
pattern
detection
University
of
Michigan
Eisner
Jason
Deep
Learning
of
Recursive
Structure
Grammar
Induction
AI
Is
Easy
to
Fool
Why
That
Needs
to
Change
Singularity
Hub
Retrieved
Gibney
Elizabeth
The
scientist
who
spots
fake
videos
Nature
doi
nature
External
links
Deep
Decision
Tree
Categories
Deep
learningArtificial
neural
networks
Navigation
menu
Not
logged
inTalkContributionsCreate
accountLog
inArticleTalkReadEditView
historySearch
Search
Wikipedia
Main
page
Contents
Featured
content
Current
events
Random
article
Donate
to
Wikipedia
Wikipedia
store
Interaction
Help
About
Wikipedia
Community
portal
Recent
changes
Contact
page
Tools
What
links
here
Related
changes
Upload
file
Special
pages
Permanent
link
Page
information
Wikidata
item
Cite
this
page
Print
export
Create
a
book
Download
as
PDF
Printable
version
Languages
Ban
lam
g
Espa
ol
Bahasa
Indonesia
Portugu
s
Ti
ng
Vi
t
more
Edit
links
This
page
was
last
edited
on
May
at
Text
is
available
under
the
Creative
Commons
Attribution
ShareAlike
License
additional
terms
may
apply
By
using
this
site
you
agree
to
the
Terms
of
Use
and
Privacy
Policy
Wikipedia
is
a
registered
trademark
of
the
Wikimedia
Foundation
Inc
a
non
profit
organization
Privacy
policyAbout
WikipediaDisclaimersContact
WikipediaDevelopersCookie
statementMobile
viewWikimedia
Foundation
Powered
by
MediaWiki
